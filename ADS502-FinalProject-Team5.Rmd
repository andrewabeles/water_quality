---
title: "ADS502 Team 5 Final Project"
author: "Katie Hu, Andrew Abeles, Emma Oo, Jake Burnett"
date: "8/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries and Dataset
```{r}
#Import Libraries and Dataset
library(knitr)
library(kableExtra)
library(ggplot2)
library(GGally)
library(class)
library(C50)
library(reldist)
library(e1071)
library(caret) 
library(rpart)
library(rpart.plot)
library(pROC)
library(devtools)
library(ggeffects)
library(tidyverse)
```

# Import Dataset
```{r}
water <- read.csv('water_potability.csv')
head(water)
summary(water)
```

# Data Cleaning

### Solve for Missing Variables
#### There are three missing variables. 
- ph: 491
- Sulfate: 781
- Trihalomethanes: 162

```{r}
#Create new dataframe to preserve original dataset and assign new variable.
water_clean <- water
```

```{r}
#Find variables with missing values.
colSums(is.na(water_clean))
```

```{r}
#See if any variables are highly correlated. 
c <- ggcorr(water_clean[,-10], label = TRUE, label_alpha = TRUE, label_round = 2, hjust = 0.8)
c + ggplot2::labs(title = 'Correlation Matrix')
```

```{r}
#Visualize Distribution of Predictor Variables
par(mfrow=c(2,3))
colnames <- dimnames(water)[[2]]
for (i in 1:9) {
    hist(water[,i],main=colnames[i], col="blue", border="white")
}
```

No variables have significant correlation with one another. Each variable with missing values has a normal distribution. With this, the missing values were replaced with the mean.

```{r}
#Replace with means

water_clean$ph[is.na(water_clean$ph)] <- mean(water_clean$ph, na.rm = TRUE)
water_clean$Sulfate[is.na(water_clean$Sulfate)] <- mean(water_clean$Sulfate, na.rm = TRUE)
water_clean$Trihalomethanes[is.na(water_clean$Trihalomethanes)] <- mean(water_clean$Trihalomethanes, na.rm = TRUE)
```

```{r, echo = FALSE, message = FALSE, results = 'hide'}
#Boxplot visual to understand shape and search for outliers
attach(water_clean)
par(mfrow=c(1,3))
boxplot(ph~Potability,data=water_clean, main="Potability and ph", xlab="Potability", ylab="ph")
boxplot(Hardness~Potability,data=water_clean, main="Potability and Hardness", xlab="Potability", ylab="Hardness")
boxplot(Solids~Potability,data=water_clean, main="Potability and Solids", xlab="Potability", ylab="Solids")
boxplot(Chloramines~Potability,data=water_clean, main="Potability and Chloramines", xlab="Potability", ylab="Chloramines")
boxplot(Sulfate~Potability,data=water_clean, main="Potability and Sulfate", xlab="Potability", ylab="Sulfate")
boxplot(Conductivity~Potability,data=water_clean, main="Potability and Conductivity", xlab="Potability", ylab="Conductivity")
boxplot(Organic_carbon~Potability,data=water_clean, main="Potability and Organic Carbon", xlab="Potability", ylab="Organic Carbon")
boxplot(Trihalomethanes~Potability,data=water_clean, main="Potability and Trihalomethanes", xlab="Potability", ylab="Trihalomethanes")
boxplot(Turbidity~Potability,data=water_clean, main="Potability and Turbidity", xlab="Potability", ylab="Turbidity")
```



## Solve for Outliers

#### Identify Outliers
```{r}
water_clean$Index <- 1:nrow(water_clean)
water_clean$Potability <- as.factor(water_clean$Potability)

z <- function(x, m, sd) { # transforms a single value into its z-score
  return((x - m) / sd)}

is_outlier <- function(x, m, sd) { # determines whether a single value is an outlier
  z <- z(x, m, sd)
  return(abs(z) > 3)}

outliers <- function(v) { # returns a boolean mask indicating which values in the vector are outliers
  m <- mean(v, na.rm = TRUE)
  sd <- sd(v, na.rm = TRUE)
  return(lapply(v, is_outlier, m=m, sd=sd))}
```

#### Outliers Identified by Attributes
```{r}
outlier_indices <- c() # vector to store outlier indices 
for (col in colnames(water_clean[, 1:9])) { # for each predictor column 
  v <- unlist(water_clean[c(col)]) # unlist and call it v 
  indices <- filter(water_clean, outliers(v) == TRUE)[c('Index')] # get its outliers' indices 
  outlier_indices <- c(outlier_indices, indices) # append them to the global vector of outlier indices
}
outlier_indices <- unlist(outlier_indices) # flatten the list of lists into a simple vector 
water_clean$Outlier <- water_clean$Index %in% outlier_indices # use the indices to create an outlier indicator column 
water_clean_outliers <- filter(water_clean, Outlier == TRUE)
water_clean_no_outliers <- filter(water_clean, Outlier == FALSE)
```

There are 147 outliers.

```{r}
#Potability Table
t <- prop.table(table(water_clean$Potability, water_clean$Outlier))
row.names(t) <- c('Not Potable', 'Potable')
colnames(t) <- c('Not Outlier', 'Outlier')
t <- addmargins(A = t, FUN = list(Total = sum), quiet = TRUE)
round(t,2)
```

The 147 outliers make up 4.49% of the data set. The outliers' Potability class balance is almost the inverse of the non-outliers'.

```{r}
#Determine if still any missing variables.
colSums(is.na(water_clean))
```


#### Save as File
```{r}
write.csv(df, "water_clean.csv", row.names = FALSE)
```

# Models

## K-Nearest Neighbor Classification 

#### Train Test Split (Including Outliers)
```{r}
set.seed(7)
n <- dim(water_clean)[1]
idx <- runif(n) < 0.8
water_train <- water_clean[ idx, ]
water_test <- water_clean[ !idx, ]
```

#### Scale Data (Including Outliers)

```{r}
y_train <- water_train$Potability
X_train <- water_train[, 1:9]
X_train <- as.data.frame(scale(X_train))
y_test <- water_test$Potability
X_test <- water_test[, 1:9]
X_test <- as.data.frame(scale(X_test))
baseline_accuracy <- length(y_test[ y_test == 0 ]) / length(y_test) 
baseline_accuracy
```

### KNN (Including Outliers)
```{r}
set.seed(7)
accuracies <- c() 
for (k in 1:10) { # for each value of K between 1 and 10
  knn <- knn( # train a KNN model 
    train = X_train, 
    test = X_test, 
    cl = y_train, 
    k = k
  ) 
  cm <- as.matrix(table(Actual = y_test, Predict = knn)) # get its confusion matrix 
  accuracy <- sum(diag(cm)) / length(y_test) # derive its accuracy from its confusion matrix
  accuracies <- append(accuracies, accuracy) # append its accuracy to the list 
}
plot( # plot the accuracy for each value of K 
  x = 1:10, 
  y = accuracies,
  type = "l",
  xlab = "K",
  ylab = "Accuracy",
  main = "k-NN Accuracies"
)
```

K = 7 seems to be the best model in terms of accuracy. Let's train a KNN model with K = 7 and calculate its confusion matrix. 

```{r}
knn <- knn(
  train = X_train,
  test = X_test, 
  cl = y_train, 
  k = 7
)
cm <- as.matrix(table(Actual = y_test, Predicted = knn))
cm <- addmargins(A = cm, FUN = list(Total = sum), quiet = TRUE)
cm
```

Let's use the confusion matrix to derive performance metrics. 
```{r}
GT <- cm[3, 3] # grand total 
TP <- cm[2, 2] # true positives
TN <- cm[1, 1] # true negatives
FP <- cm[1, 2] # false positives 
FN <- cm[2, 1] # false negatives
TAP <- cm[2, 3] # total actual positives 
TAN <- cm[1, 3] # total actual negatives 
TPP <- cm[3, 2] # total predicted positives 
TPN <- cm[3, 1] # total predicted negatives 
```

```{r}
accuracy <- (TP + TN) / GT
precision <- TP / TPP
recall <- TP / TAP 
specificity <- TN / TAN 
F1_score <- 2 * (precision * recall) / (precision + recall)
round(accuracy,2)
round(precision,2)
round(recall,2) 
round(specificity,2) 
round(F1_score,2)
```


### KNN (Excluding Outliers)

#### Train Test Split
```{r}
water_clean_no_outliers <- filter(water_clean, Outlier == FALSE)
set.seed(7)
n <- dim(water_clean_no_outliers)[1]
idx <- runif(n) < 0.8
water_train <- water_clean_no_outliers[ idx, ]
water_test <- water_clean_no_outliers[ !idx, ]
```

#### Scale Data 
```{r}
y_train <- water_train$Potability
X_train <- water_train[, 1:9]
X_train <- as.data.frame(scale(X_train))
y_test <- water_test$Potability
X_test <- water_test[, 1:9]
X_test <- as.data.frame(scale(X_test))
baseline_accuracy <- length(y_test[ y_test == 0 ]) / length(y_test) 
baseline_accuracy
```

```{r}
set.seed(7)
accuracies <- c() 
for (k in 1:10) { # for each value of K between 1 and 10
  knn <- knn( # train a KNN model 
    train = X_train, 
    test = X_test, 
    cl = y_train, 
    k = k
  ) 
  cm <- as.matrix(table(Actual = y_test, Predict = knn)) # get its confusion matrix 
  accuracy <- sum(diag(cm)) / length(y_test) # derive its accuracy from its confusion matrix
  accuracies <- append(accuracies, accuracy) # append its accuracy to the list 
}
plot( # plot the accuracy for each value of K 
  x = 1:10, 
  y = accuracies,
  type = "l",
  xlab = "K",
  ylab = "Accuracy",
  main = "k-NN Accuracies"
)
```

K = 9 seems to be the best model in terms of accuracy. Let's train a KNN model with K = 9 and calculate its confusion matrix. 

```{r}
knn <- knn(
  train = X_train,
  test = X_test, 
  cl = y_train, 
  k = 9
)
cm <- as.matrix(table(Actual = y_test, Predicted = knn))
cm <- addmargins(A = cm, FUN = list(Total = sum), quiet = TRUE)
cm
```

Let's use the confusion matrix to derive performance metrics. 
```{r}
GT <- cm[3, 3] # grand total 
TP <- cm[2, 2] # true positives
TN <- cm[1, 1] # true negatives
FP <- cm[1, 2] # false positives 
FN <- cm[2, 1] # false negatives
TAP <- cm[2, 3] # total actual positives 
TAN <- cm[1, 3] # total actual negatives 
TPP <- cm[3, 2] # total predicted positives 
TPN <- cm[3, 1] # total predicted negatives 
```

```{r}
accuracy <- (TP + TN) / GT
precision <- TP / TPP
recall <- TP / TAP 
specificity <- TN / TAN 
F1_score <- 2 * (precision * recall) / (precision + recall)
round(accuracy,2)
round(precision,2)
round(recall,2) 
round(specificity,2) 
round(F1_score,2)
```

The model performs better when outliers are excluded. 

## Decision Tree Cart & C5.0

#### Splitting into training and test data (80:20 ratio)

```{r}
set.seed(7)
n <- dim(water_clean)[1]
water_ind <- runif(n) < 0.80
water_train <- water_clean[ water_ind, ]
water_test <- water_clean[ !water_ind, ]
#Checking how many records(rows) in training and test data set
table(water_train$Potability)
```

#### Getting Gini Impurity for Decision Tree

```{r}
round(gini(water_train$ph),2)   #1
round(gini(water_train$Hardness),2)  #2
round(gini(water_train$Solids),2)  #3
round(gini(water_train$Chloramines),2)  #4
round(gini(water_train$Sulfate),2)  #5
round(gini(water_train$Conductivity),2)  #6 
round(gini(water_train$Organic_carbon),2)  #7
round(gini(water_train$Trihalomethanes),2)  #8
round(gini(water_train$Turbidity),2)  #9
```

#### Sulfate has Lowest Gini (0.058)

```{r}
cart01 <- rpart(formula = Potability ~  ph + Hardness + Sulfate,  data=water_train, method = "class")
rpart.plot(cart01, main = 'Cart Decision Tree')
rpart.plot(cart01, type = 4, extra = 4, main = 'Cart Decision Tree with Split Labels')
```

#### Decision Tree Interpretation

Nodes are split by the sulfate, ph, and hardness (since they have lowest GINI index) among all other variables. 
39% of the training data set have sulfate level <258 which shows potability of water (1). Only 3% of the training data set. 
Another 97% has sulfate > 258.  That node is then split again with sulfate level of 388. 
Sulfate < 388 and ph < 7.6 are still potable while ph > 7.6 falls into potability of 0.
Sulfate > 388, hardness < 162, ph > 7 are potablility of 1.  

#### To obtain classifications for each record in data set using CART model

```{r}
X = data.frame(ph = water_train$ph, Hardness = water_train$Hardness, Solids = water_train$Solids, Chloramines = water_train$Chloramines, Sulfate = water_train$Sulfate, Conductivity = water_train$Conductivity, Organic_carbon = water_train$Organic_carbon, Trihalomethanes = water_train$Trihalomethanes, Turbidity =water_train$Turbidity) 

predPotabilityCART = predict(object =cart01, newdata = X, type="class")
```

#### Evaluating the Model

```{r}
test.X <- subset (x= water_test, select = c("ph", "Hardness", "Solids", "Chloramines", "Sulfate", "Conductivity", "Organic_carbon", "Trihalomethanes", "Turbidity"))
ypred <- predict(object = cart01, newdata = test.X, type = "class")

t1 <- table(water_test$Potability, ypred)
row.names(t1) <- c("Actual:0", "Actual: 1")
colnames(t1) <- c("Predicted: 0", "Predicted:1")
t1 <- addmargins(A=t1,FUN=list(Total = sum), quiet = TRUE); t1
```

#### Checking Overrall Statistics of Model

```{r}
predicted.classes <- ypred
observed.classes <- water_test$Potability
accuracy <- mean(observed.classes == predicted.classes)
accuracy

predicted.classes <- factor(predicted.classes)
observed.classes <- factor(observed.classes)

confusionmatrix <- confusionMatrix(data=predicted.classes, reference = observed.classes, positive = '1')
confusionmatrix
```

#### Testing out pre-pruning to get better accuracy score

```{r}
print(cart01) #text version of tree
printcp(cart01) 
plotcp(cart01, main = "CP Values") #Plots multiple cp values with error - only works if using cross-validation (xval > 0)
```

#### Pre-Pruning

```{r}
# Grow a tree with minsplit of 100 and max depth of 8

cart01_preprun <- rpart(formula = Potability ~  ph + Sulfate + Hardness,  data=water_train, method = "class", control = rpart.control(cp = 0, maxdepth = 8, minsplit = 100))

printcp(cart01_preprun)
rpart.plot(cart01_preprun, main = "Pre-Pruning Cart Decision Tree")
rpart.plot(cart01_preprun, type = 4, extra = 2, main = "Pre-Pruning Cart Decision Tree with Split Labels")
```

#### To obtain classifications for each record in data set using CART model

```{r}
X = data.frame(ph = water_train$ph, Hardness = water_train$Hardness, Solids = water_train$Solids, Chloramines = water_train$Chloramines, Sulfate = water_train$Sulfate, Conductivity = water_train$Conductivity, Organic_carbon = water_train$Organic_carbon, Trihalomethanes = water_train$Trihalomethanes, Turbidity =water_train$Turbidity)

predPotabilityCART = predict(object = cart01, newdata = X, type="class")
```

#### Evaluating the Model

```{r}
test.X <- subset (x= water_test, select = c("ph", "Hardness", "Solids", "Chloramines", "Sulfate", "Conductivity", "Organic_carbon", "Trihalomethanes", "Turbidity"))

ypred <- predict(object = cart01_preprun, newdata = test.X, type = "class")

t1 <- table(water_test$Potability, ypred)
row.names(t1) <- c("Actual:0", "Actual: 1")
colnames(t1) <- c("Predicted: 0", "Predicted:1")
t1 <- addmargins(A=t1,FUN=list(Total = sum), quiet = TRUE); t1
```

#### Pre-Pruning Accuracy

```{r}
# Compute the Accuracy of the Pruned Tree
ypred_prun <- predict(object = cart01_preprun, newdata = test.X, type = "class")

predicted.classes_prun <- ypred_prun
observed.classes_prun <- water_test$Potability
accuracy_prun <- mean(observed.classes == predicted.classes_prun)
accuracy_prun

predicted.class_prun <- factor(predicted.classes_prun)
observed.classes_prun <- factor(observed.classes_prun)

confusionmatrix_prun <- confusionMatrix(data = predicted.classes_prun, reference = observed.classes_prun, positive ="1")
confusionmatrix_prun
```

### Decision Tree using C5.0

```{r}
water_train$Potability <- factor(water_train$Potability)
C5 <- C5.0(formula = Potability ~ Sulfate + Hardness + ph, data = water_train, control = C5.0Control(minCases=75))
plot(C5, main = 'C5.0 Decision Tree')
```

#### To obtain classifications for each record in data set using CART model

```{r}
X = data.frame(ph = water_train$ph, Hardness = water_train$Hardness, Solids = water_train$Solids, Chloramines = water_train$Chloramines, Sulfate = water_train$Sulfate, Conductivity = water_train$Conductivity, Organic_carbon = water_train$Organic_carbon, Trihalomethanes = water_train$Trihalomethanes, Turbidity =water_train$Turbidity)

pred <- predict(object = C5, newdata = X)
```

#### Evaluating the Model

```{r}
test.X <- subset (x= water_test, select = c("ph", "Hardness", "Solids", "Chloramines", "Sulfate", "Conductivity", "Organic_carbon", "Trihalomethanes", "Turbidity"))

ypred <- predict(object = C5, newdata = test.X, type = "class")

t1 <- table(water_test$Potability, ypred)
row.names(t1) <- c("Actual:0", "Actual: 1")
colnames(t1) <- c("Predicted: 0", "Predicted:1")
t1 <- addmargins(A=t1,FUN=list(Total = sum), quiet = TRUE); t1
```

#### Checking Overrall Statistics of Model

```{r}
predicted.classes <- ypred
observed.classes <- water_test$Potability
accuracy <- mean(observed.classes == predicted.classes)
accuracy

predicted.classes <- factor(predicted.classes)
observed.classes <- factor(observed.classes)

confusionmatrix <- confusionMatrix(data=predicted.classes, reference = observed.classes, positive = '1')
confusionmatrix
```

### Decision Tree Models Without Outliers

```{r}
set.seed(7)

n <- dim(water_clean_no_outliers)[1]

water_ind_no_outliers <- runif(n) < 0.8

water_train_no_outliers <- water_clean_no_outliers[ water_ind_no_outliers, ]
water_test_no_outliers <- water_clean_no_outliers[ !water_ind_no_outliers, ]
```

### CART

```{r}
cart_nooutliers <- rpart(formula = Potability ~  ph + Hardness +  Sulfate, 
data=water_train_no_outliers, method = "class")

rpart.plot(cart_nooutliers, main = "Cart Decision Tree with No Outliers")
rpart.plot(cart_nooutliers, type = 4, extra = 4, main = "Cart Decision Tree with Split Levels and No Outliers")
```

```{r}
X = data.frame(ph = water_train_no_outliers$ph, Hardness = water_train_no_outliers$Hardness, Solids = water_train_no_outliers$Solids, Chloramines = water_train_no_outliers$Chloramines, Sulfate = water_train_no_outliers$Sulfate, Conductivity = water_train_no_outliers$Conductivity, Organic_carbon = water_train_no_outliers$Organic_carbon, Trihalomethanes = water_train_no_outliers$Trihalomethanes, Turbidity =water_train_no_outliers$Turbidity) 

predPotabilityCART_nooutliers = predict(object = cart_nooutliers, newdata = X, type="class")
```

#### Evaluating the CART Model without Outliers

```{r}
test.X_nooutliers <- subset (x= water_test_no_outliers, select = c("ph", "Hardness", "Solids", "Chloramines", "Sulfate", "Conductivity", "Organic_carbon", "Trihalomethanes", "Turbidity"))


ypred_nooutliers <- predict(object = cart_nooutliers, newdata = test.X_nooutliers, type = "class")

t11 <- table(water_test_no_outliers$Potability, ypred_nooutliers)
row.names(t11) <- c("Actual:0", "Actual: 1")
colnames(t11) <- c("Predicted: 0", "Predicted:1")
t11 <- addmargins(A=t1,FUN=list(Total = sum), quiet = TRUE); t1
```

#### Checking Overrall Statistics of CART Model

```{r}
predicted.classes_nooutliers <- ypred_nooutliers
observed.classes_nooutliers <- water_test_no_outliers$Potability
accuracy_nooutliers <- mean(observed.classes_nooutliers == predicted.classes_nooutliers)
accuracy_nooutliers


predicted.classes_nooutliers <- factor(predicted.classes_nooutliers)
observed.classes_nooutliers <- factor(observed.classes_nooutliers)

confusionmatrix_nooutliers <- confusionMatrix(data=predicted.classes_nooutliers, reference = observed.classes_nooutliers, positive='1')

confusionmatrix_nooutliers
```

#### Pre-Pruning without Outliers for CART

```{r}
# Grow a tree with minsplit of 100 and max depth of 8

cart01_preprun_nooutliers <- rpart(formula = Potability ~  Sulfate + Hardness + ph, data=water_train_no_outliers, method = "class", control = rpart.control(cp = 0, maxdepth = 8,minsplit = 100))

printcp(cart01_preprun_nooutliers)
rpart.plot(cart01_preprun_nooutliers, main = "Pre-Pruning Cart Decision Tree without Outliers")
rpart.plot(cart01_preprun_nooutliers, type = 4, extra = 2, main = "Pre-Pruning Cart Decision Tree with Split Level and without Outliers ")
```

#### Checking Overall Statistics of Pre-Pruned CART

```{r}
# Compute the Accuracy of the Pruned Tree

ypred_prun_nooutliers <- predict(object = cart01_preprun_nooutliers, newdata = test.X_nooutliers, type = "class")

predicted.classes_prun_nooutliers <- ypred_prun_nooutliers
observed.classes_prun_nooutliers <- water_test_no_outliers$Potability
accuracy_prun_nooutliers <- mean(observed.classes_nooutliers == predicted.classes_prun_nooutliers)
accuracy_prun_nooutliers


predicted.classes_prun_nooutliers <- factor(predicted.classes_prun_nooutliers)
observed.classes_prun_nooutliers <- factor(observed.classes_prun_nooutliers)

confusionmatrix_prun_nooutliers <- confusionMatrix(data=predicted.classes_prun_nooutliers, reference = observed.classes_prun_nooutliers, positive = "1")

confusionmatrix_prun_nooutliers
```

### Decision tree using C5.0 without outliers

```{r}
water_train_no_outliers$Potability <- factor(water_train_no_outliers$Potability)
C5_noout <- C5.0(formula = Potability ~ Sulfate + Hardness + ph ,  data=water_train_no_outliers, control = C5.0Control(minCases=75))
plot(C5_noout, main = "C5.0 Decision Tree without Outliers")
```

#### To obtain classifications for each record in data set using CART model

```{r}
X_C5.0 = data.frame(ph = water_train_no_outliers$ph, Hardness = water_train_no_outliers$Hardness, Solids = water_train_no_outliers$Solids, Chloramines = water_train_no_outliers$Chloramines, Sulfate = water_train_no_outliers$Sulfate, Conductivity = water_train_no_outliers$Conductivity, Organic_carbon = water_train_no_outliers$Organic_carbon, Trihalomethanes = water_train_no_outliers$Trihalomethanes, Turbidity =water_train_no_outliers$Turbidity) 

pred_wo <- predict(object = C5_noout, newdata = X_C5.0)
```

#### Evaluating the C5.0 model

```{r}
test.X_C5.0 <- subset (x= water_test_no_outliers, select = c("ph", "Hardness", "Solids", "Chloramines", "Sulfate", "Conductivity", "Organic_carbon", "Trihalomethanes", "Turbidity"))


ypred_C5.0 <- predict(object = C5_noout, newdata = test.X_C5.0, type = "class")

t13 <- table(water_test_no_outliers$Potability, ypred_C5.0)
row.names(t13) <- c("Actual:0", "Actual: 1")
colnames(t13) <- c("Predicted: 0", "Predicted:1")
t13 <- addmargins(A=t1,FUN=list(Total = sum), quiet = TRUE); t1
```

#### Checking Overrall Statistics of C5.0 Model

```{r}
predicted.classes_C5.0 <- ypred_C5.0
observed.classes_C5.0 <- water_test_no_outliers$Potability
accuracy_C5.0 <- mean(observed.classes_C5.0 == predicted.classes_C5.0)
accuracy_C5.0


predicted.classes_C5.0 <- factor(predicted.classes_C5.0)
observed.classes_C5.0 <- factor(observed.classes_C5.0)

confusionmatrix_C5.0 <- confusionMatrix(data=predicted.classes_C5.0, reference = observed.classes_C5.0, positive = "1")

confusionmatrix_C5.0
```

## Logistic Regression

```{r}
set.seed(7)
```

```{r}
n <- dim(water_clean)[1]
```

```{r}
water_ind <- runif(n) < 0.8
```

```{r}
water_train <- water_clean[ water_ind, ]
water_test <- water_clean[ !water_ind, ]
```

```{r}
#Binomial logistic regression for training data set
logreg01 <- glm(formula = Potability ~ ph + Hardness + Solids + Chloramines + Sulfate + Conductivity + Organic_carbon + Trihalomethanes + Turbidity, data = water_train, family = binomial)
summary(logreg01)
```

```{r}
#Binomial logistic regression for test data set
logreg01_test <- glm(formula = Potability ~ ph + Hardness + Solids + Chloramines + Sulfate + Conductivity + Organic_carbon + Trihalomethanes + Turbidity, data = water_test, family = binomial)
summary(logreg01_test)
```

```{r}
#Binomial logistic regression for train data set with select variables
logreg02 <- glm(formula = Potability ~ Solids + Organic_carbon, data = water_train, family = binomial)
summary(logreg02)
```

```{r}
#Binomial logistic regression for test data set select variables
logreg02_test <- glm(formula = Potability ~ Solids + Organic_carbon, data = water_test, family = binomial)
summary(logreg02_test)
```

```{r}
entry01 <- data.frame(Solids = 20791.319, Organic_carbon = 10.379783)
predict(object = logreg02, newdata = entry01)
```
```{r}
predicted <- predict(logreg02, newdata = water_test, type = "response")
```

```{r}
#Logistic Regression Plot of predicted probabilities for potability
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
lg <- plot(ggpredict(logreg02_test, c('Solids','Organic_carbon'))) + scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)
lg + ggplot2::labs(title = 'Probabilities of Potability')
```

```{r}
table(water_test$Potability)
```

```{r}
cmatrix <- table(water_test$Potability, predicted < 0.378)
cmatrix <- addmargins(A = cmatrix, FUN = list(Total = sum), quiet = TRUE)
cmatrix
```

```{r}
#Matrices set up for metrics

GT <- cmatrix[3, 3]
TP <- cmatrix[2, 2]
TN <- cmatrix[1, 1]
FP <- cmatrix[1, 2]
FN <- cmatrix[2, 1]
TAP <- cmatrix[2, 3]
TAN <- cmatrix[1, 3]
TPP <- cmatrix[3, 2]
TPN <- cmatrix[3, 1]
```

```{r}
#Metrics 
accuracy <- (TP + TN) / GT
precision <- TP / TPP
sensitivity <- TP / TAP
specificity <- TN / TAN
round(accuracy,2)
round(precision,2)
round(sensitivity,2)
round(specificity,2)
```


## Logistic Regression with No Outliers

```{r}
set.seed(7)
```

```{r}
n <- dim(water_clean_no_outliers)[1]
```

```{r}
water_ind_no_outliers <- runif(n) < 0.8
```

```{r}
water_train_no_outliers <- water_clean_no_outliers[ water_ind_no_outliers, ]
water_test_no_outliers <- water_clean_no_outliers[ !water_ind_no_outliers, ]
```

```{r}
#logistic regression training data set summary of all variables
logreg01_no_outliers <- glm(formula = Potability ~ ph + Hardness + Solids + Chloramines + Sulfate + Conductivity + Organic_carbon + Trihalomethanes + Turbidity, data = water_train_no_outliers, family = binomial)
summary(logreg01_no_outliers)
```

```{r}
#logistic regression test set summary with all variables
logreg01_test_no_outliers <- glm(formula = Potability ~ ph + Hardness + Solids + Chloramines + Sulfate + Conductivity + Organic_carbon + Trihalomethanes + Turbidity, data = water_test_no_outliers, family = binomial)
summary(logreg01_test_no_outliers)
```

```{r}
#logistic regression train data set with select variables
logreg02_no_outliers <- glm(formula = Potability ~ Solids + Organic_carbon, data = water_train_no_outliers, family = binomial)
summary(logreg02_no_outliers)
```

```{r}
#logistic regression test set summary with select variables
logreg02_test_no_outliers <- glm(formula = Potability ~ Solids + Organic_carbon, data = water_test_no_outliers, family = binomial)
summary(logreg02_test_no_outliers)
```
After analyzing the summary with all predictor variables and with select variables, the select variables produced better results. With this, the remaining predictions will move forward with the logistic regression using select variables of *solids* and *organic_carbon*.

```{r}
entry01_no_outliers <- data.frame(Solids = 20791.319, Organic_carbon = 10.379783)
round(predict(object = logreg02_no_outliers, newdata = entry01_no_outliers),2)
```
```{r}
predicted_no_outliers <- predict(logreg02_no_outliers, newdata = water_test_no_outliers, type = "response")
```

```{r}
table(water_test_no_outliers$Potability)
```

```{r}
cmatrix_no_outliers <- table(water_test_no_outliers$Potability, predicted_no_outliers < 0.378)
cmatrix_no_outliers <- addmargins(A = cmatrix_no_outliers, FUN = list(Total = sum), quiet = TRUE)
cmatrix_no_outliers
```

```{r}
GT_no_outliers <- cmatrix_no_outliers[3, 3]
TP_no_outliers <- cmatrix_no_outliers[2, 2]
TN_no_outliers <- cmatrix_no_outliers[1, 1]
FP_no_outliers <- cmatrix_no_outliers[1, 2]
FN_no_outliers <- cmatrix_no_outliers[2, 1]
TAP_no_outliers <- cmatrix_no_outliers[2, 3]
TAN_no_outliers <- cmatrix_no_outliers[1, 3]
TPP_no_outliers <- cmatrix_no_outliers[3, 2]
TPN_no_outliers <- cmatrix_no_outliers[3, 1]
```

```{r}
#Metrics 
accuracy <- (TP_no_outliers + TN_no_outliers) / GT_no_outliers
precision <- TP_no_outliers / TPP_no_outliers
sensitivity <- TP_no_outliers / TAP_no_outliers
specificity <- TN_no_outliers / TAN_no_outliers
round(accuracy,2)
round(precision,2)
round(sensitivity,2)
round(specificity,2)
```

```{r}
model_metrics = data.frame(
  Metrics = c("Accuracy", "Precision", "Sensitivity", "Specificity"),
  k_NN = c(".65", ".62", ".34", ".86"),
  DT_CART = c(".62", ".58", ".92", ".19"),
  DT_C5.0 = c(".63", ".64", ".17", ".94"),
  LR = c(".53", ".41", ".41", ".61")
)
```

```{r}
#kable(
  #model_metrics,
  #format = "html",
  #booktabs = TRUE,
  #col.names = c("Metrics", "k-NN", "CART", "C5.0", "Logistic Regression"),
  #align = c("l", "c", "c", "c"),
  #caption = "Model Metrics"
  #)
```
```{r}
kable(
  model_metrics,
  format = "html",
  col.names = c("Metrics", "k-NN", "CART", "C5.0", "Logistic Regression"),
  align = c("l", "c", "c", "c"),
  caption = "Model Metrics"
  ) %>%
  kable_classic(full_width = TRUE, html_font = "Times New Roman") %>%
  row_spec(row = 0, align = "c") 
```
```{r}
kable(
  (model_metrics[1:5, 1:5]),
  format = "html",
  col.names = c("Metrics", "k-NN", "CART", "C5.0", "Logistic Regression"),
  align = c("l", "c", "c", "c"),
  caption = "Model Metrics"
  ) %>%
  kable_classic(full_width = TRUE, html_font = "Times New Roman") %>%
  row_spec(row = 0, align = "c") %>%
  pack_rows('With Outliers', 1, 4) 
```


