---
title: "Water Potability Classification"
author: "Andrew Abeles"
date: "8/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(class)
```

```{r}
df <- read.csv("water_potability.csv")
df$Index <- 1:nrow(df)
df$Potability <- as.factor(df$Potability)
head(df)
```
### Outliers 

Let's create some functions to help us identify outliers. 

```{r}
z <- function(x, m, sd) { # transforms a single value into its z-score 
  return((x - m) / sd)
}

is_outlier <- function(x, m, sd) { # determines whether a single value is an outlier
  z <- z(x, m, sd)
  return(abs(z) > 3)
}

outliers <- function(v) { # returns a boolean mask indicating which values in the vector are outliers
  m <- mean(v, na.rm = TRUE)
  sd <- sd(v, na.rm = TRUE)
  return(lapply(v, is_outlier, m=m, sd=sd))
}
```

Let's use these functions to label an instance as an outlier if any of its attributes is an outlier. 

```{r}
outlier_indices <- c() # vector to store outlier indices 
for (col in colnames(df[, 1:9])) { # for each predictor column 
  v <- unlist(df[c(col)]) # unlist and call it v 
  indices <- filter(df, outliers(v) == TRUE)[c("Index")] # get its outliers' indices 
  outlier_indices <- c(outlier_indices, indices) # append them to the global vector of outlier indices 
}
outlier_indices <- unlist(outlier_indices) # flatten the list of lists into a simple vector 
df$Outlier <- df$Index %in% outlier_indices # use the indices to create an outlier indicator column 
df_outliers <- filter(df, Outlier == TRUE)
df_no_outliers <- filter(df, Outlier == FALSE)
df_outliers
```

There are 115 outliers. 

```{r}
t <- prop.table(table(df$Potability, df$Outlier))
row.names(t) <- c("Not Potable", "Potable")
colnames(t) <- c("Not Outlier", "Outlier")
t <- addmargins(A = t, FUN = list(Total = sum), quiet = TRUE)
t
```

The 115 outliers make up 3.5% of the dataset. The outliers' Potability class balance is almost the inverse of the non-outliers'.  

### Missing Values

```{r}
colSums(is.na(df))
```
ph, Sulfate, and Trihalomethanes have missing values. They are not correlated with other attributes and their distributions are approximately normal so let's just replace their missing values with their non-outlier means. 

```{r}
df$ph[is.na(df$ph)] <- mean(df_no_outliers$ph, na.rm = TRUE)
df$Sulfate[is.na(df$Sulfate)] <- mean(df_no_outliers$Sulfate, na.rm = TRUE)
df$Trihalomethanes[is.na(df$Trihalomethanes)] <- mean(df_no_outliers$Trihalomethanes, na.rm = TRUE)
colSums(is.na(df))
```

There are no more missing values. 

```{r}
write.csv(df, "water_potability_clean.csv", row.names = FALSE)
```

### Train Test Split 
```{r}
set.seed(7)
n <- dim(df)[1]
idx <- runif(n) < 0.8
df_train <- df[ idx, ]
df_test <- df[ !idx, ]
```

### Scale Data

```{r}
y_train <- df_train$Potability
X_train <- df_train[, 1:9]
X_train <- as.data.frame(scale(X_train))
y_test <- df_test$Potability
X_test <- df_test[, 1:9]
X_test <- as.data.frame(scale(X_test))
baseline_accuracy <- length(y_test[ y_test == 0 ]) / length(y_test) 
baseline_accuracy
```

### Model
#### KNN
```{r}
set.seed(7)
accuracies <- c() 
for (k in 1:10) { # for each value of K between 1 and 10
  knn <- knn( # train a KNN model 
    train = X_train, 
    test = X_test, 
    cl = y_train, 
    k = k
  ) 
  cm <- as.matrix(table(Actual = y_test, Predict = knn)) # get its confusion matrix 
  accuracy <- sum(diag(cm)) / length(y_test) # derive its accuracy from its confusion matrix
  accuracies <- append(accuracies, accuracy) # append its accuracy to the list 
}
plot( # plot the accuracy for each value of K 
  x = 1:10, 
  y = accuracies,
  type = "l",
  xlab = "K",
  ylab = "Accuracy",
  main = "KNN Accuracies"
)
```

K = 5 seems to be the best model in terms of accuracy. Let's train a KNN model with K = 5 and calculate its confusion matrix. 

```{r}
knn <- knn(
  train = X_train,
  test = X_test, 
  cl = y_train, 
  k = 5
)
cm <- as.matrix(table(Actual = y_test, Predicted = knn))
cm <- addmargins(A = cm, FUN = list(Total = sum), quiet = TRUE)
cm
```

Let's use the confusion matrix to derive performance metrics. 
```{r}
GT <- cm[3, 3] # grand total 
TP <- cm[2, 2] # true positives
TN <- cm[1, 1] # true negatives
FP <- cm[1, 2] # false positives 
FN <- cm[2, 1] # false negatives
TAP <- cm[2, 3] # total actual positives 
TAN <- cm[1, 3] # total actual negatives 
TPP <- cm[3, 2] # total predicted positives 
TPN <- cm[3, 1] # total predicted negatives 
```

```{r}
accuracy <- (TP + TN) / GT
precision <- TP / TPP
recall <- TP / TAP 
specificity <- TN / TAN 
F1_score <- 2 * (precision * recall) / (precision + recall)
accuracy
precision
recall 
specificity 
F1_score
```































